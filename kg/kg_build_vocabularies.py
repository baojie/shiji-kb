#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»æ ‡æ³¨çš„å²è®°ç« èŠ‚ä¸­æå–åˆ†ç±»è¯è¡¨
"""

import os
import re
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

# å®šä¹‰11ç±»å®ä½“çš„æ ‡è®°æ¨¡å¼
ENTITY_PATTERNS = {
    'äººå': (r'@([^@]+)@', '@'),
    'åœ°å': (r'=([^=]+)=', '='),
    'å®˜èŒ': (r'\$([^\$]+)\$', '$'),
    'æ—¶é—´': (r'%([^%]+)%', '%'),
    'æœä»£': (r'&([^&]+)&', '&'),
    'åˆ¶åº¦': (r'\^([^\^]+)\^', '^'),
    'æ—ç¾¤': (r'~([^~]+)~', '~'),
    'å™¨ç‰©': (r'\*([^\*]+)\*', '*'),
    'å¤©æ–‡': (r'!([^!]+)!', '!'),
    'ç¥è¯': (r'\?([^\?]+)\?', '?'),
    'åŠ¨æ¤ç‰©': (r'ğŸŒ¿([^ğŸŒ¿]+)ğŸŒ¿', 'ğŸŒ¿'),
}

class VocabularyBuilder:
    def __init__(self, chapter_dir: str):
        self.chapter_dir = Path(chapter_dir)
        # å­˜å‚¨: {ç±»åˆ«: {è¯æ¡: [(ç« èŠ‚å, ä¸Šä¸‹æ–‡)]}}
        self.vocabularies: Dict[str, Dict[str, List[Tuple[str, str]]]] = {
            category: defaultdict(list) for category in ENTITY_PATTERNS.keys()
        }

    def extract_entities_from_file(self, file_path: Path):
        """ä»å•ä¸ªæ–‡ä»¶ä¸­æå–å®ä½“"""
        chapter_name = file_path.stem.replace('.tagged', '')

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            lines = content.split('\n')

        for category, (pattern, marker) in ENTITY_PATTERNS.items():
            matches = re.finditer(pattern, content)

            for match in matches:
                entity = match.group(1).strip()
                if not entity:
                    continue

                # è·å–ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è¯¥å®ä½“çš„æ®µè½ï¼‰
                start_pos = match.start()

                # æ‰¾åˆ°æ‰€åœ¨è¡Œ
                context_line = None
                pos = 0
                for line in lines:
                    if pos <= start_pos < pos + len(line) + 1:
                        context_line = line.strip()
                        break
                    pos += len(line) + 1

                # æ¸…ç†ä¸Šä¸‹æ–‡ï¼Œç§»é™¤æ®µè½ç¼–å·å’Œå…¶ä»–æ ‡è®°
                if context_line:
                    # ç§»é™¤æ®µè½ç¼–å· [x.x.x]
                    context = re.sub(r'^\[[\d\.]+\]\s*', '', context_line)
                    # é™åˆ¶é•¿åº¦
                    if len(context) > 100:
                        context = context[:100] + '...'
                else:
                    context = match.group(0)

                self.vocabularies[category][entity].append((chapter_name, context))

    def process_all_files(self):
        """å¤„ç†æ‰€æœ‰tagged.mdæ–‡ä»¶"""
        tagged_files = list(self.chapter_dir.glob('*.tagged.md'))
        print(f"å‘ç° {len(tagged_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")

        for i, file_path in enumerate(tagged_files, 1):
            print(f"  [{i}/{len(tagged_files)}] å¤„ç†: {file_path.name}")
            self.extract_entities_from_file(file_path)

        print("\næå–å®Œæˆï¼")

    def generate_vocabulary_file(self, category: str, output_dir: Path):
        """ç”Ÿæˆå•ä¸ªç±»åˆ«çš„è¯è¡¨æ–‡ä»¶"""
        vocab_data = self.vocabularies[category]

        if not vocab_data:
            print(f"  {category}: 0 ä¸ªè¯æ¡ï¼ˆè·³è¿‡ï¼‰")
            return

        # æŒ‰è¯æ¡å‡ºç°æ¬¡æ•°æ’åº
        sorted_entries = sorted(vocab_data.items(),
                               key=lambda x: len(x[1]),
                               reverse=True)

        # ç”Ÿæˆæ–‡ä»¶å
        filename_map = {
            'äººå': '01_äººåè¯è¡¨.md',
            'åœ°å': '02_åœ°åè¯è¡¨.md',
            'å®˜èŒ': '03_å®˜èŒè¯è¡¨.md',
            'æ—¶é—´': '04_æ—¶é—´è¯è¡¨.md',
            'æœä»£': '05_æœä»£è¯è¡¨.md',
            'åˆ¶åº¦': '06_åˆ¶åº¦è¯è¡¨.md',
            'æ—ç¾¤': '07_æ—ç¾¤è¯è¡¨.md',
            'å™¨ç‰©': '08_å™¨ç‰©è¯è¡¨.md',
            'å¤©æ–‡': '09_å¤©æ–‡è¯è¡¨.md',
            'ç¥è¯': '10_ç¥è¯è¯è¡¨.md',
            'åŠ¨æ¤ç‰©': '11_åŠ¨æ¤ç‰©è¯è¡¨.md',
        }

        output_file = output_dir / filename_map[category]

        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥æ ‡é¢˜å’Œè¯´æ˜
            marker = ENTITY_PATTERNS[category][1]
            f.write(f"# å²è®°{category}è¯è¡¨\n\n")
            f.write(f"> æ ‡è®°ç¬¦å·ï¼š`{marker}è¯æ¡{marker}`  \n")
            f.write(f"> è¯æ¡æ€»æ•°ï¼š**{len(sorted_entries)}**  \n")
            f.write(f"> æ€»å‡ºç°æ¬¡æ•°ï¼š**{sum(len(contexts) for _, contexts in sorted_entries)}**  \n")
            f.write(f"> æ•°æ®æ¥æºï¼š{len(list(self.chapter_dir.glob('*.tagged.md')))} ä¸ªå·²æ ‡æ³¨ç« èŠ‚\n\n")
            f.write("---\n\n")

            # å†™å…¥è¯æ¡
            for i, (entity, contexts) in enumerate(sorted_entries, 1):
                f.write(f"## {i}. {entity}\n\n")
                f.write(f"**å‡ºç°æ¬¡æ•°**ï¼š{len(contexts)} æ¬¡\n\n")

                # æŒ‰ç« èŠ‚åˆ†ç»„
                chapters_dict = defaultdict(list)
                for chapter, context in contexts:
                    chapters_dict[chapter].append(context)

                f.write(f"**å‡ºç°ç« èŠ‚**ï¼š{', '.join(sorted(set(c for c, _ in contexts)))}\n\n")

                # åˆ—å‡ºå…¸å‹ç”¨ä¾‹ï¼ˆæœ€å¤š5ä¸ªï¼‰
                f.write("**å…¸å‹ç”¨ä¾‹**ï¼š\n\n")
                unique_contexts = []
                seen = set()
                for chapter, context in contexts:
                    if context not in seen:
                        unique_contexts.append((chapter, context))
                        seen.add(context)
                    if len(unique_contexts) >= 5:
                        break

                for chapter, context in unique_contexts:
                    # é«˜äº®å½“å‰è¯æ¡
                    highlighted = context.replace(f'{marker}{entity}{marker}',
                                                 f'**{marker}{entity}{marker}**')
                    f.write(f"- ã€{chapter}ã€‘{highlighted}\n")

                f.write("\n")

        print(f"  {category}: {len(sorted_entries)} ä¸ªè¯æ¡ â†’ {output_file.name}")

    def generate_all_vocabularies(self, output_dir: Path):
        """ç”Ÿæˆæ‰€æœ‰ç±»åˆ«çš„è¯è¡¨"""
        output_dir.mkdir(parents=True, exist_ok=True)

        print("\n" + "="*60)
        print("ç”Ÿæˆè¯è¡¨æ–‡ä»¶")
        print("="*60 + "\n")

        for category in ENTITY_PATTERNS.keys():
            self.generate_vocabulary_file(category, output_dir)

        # ç”Ÿæˆæ€»ç´¢å¼•
        self.generate_index(output_dir)

    def generate_index(self, output_dir: Path):
        """ç”Ÿæˆè¯è¡¨æ€»ç´¢å¼•"""
        index_file = output_dir / 'README.md'

        with open(index_file, 'w', encoding='utf-8') as f:
            f.write("# å²è®°åˆ†ç±»è¯è¡¨ç´¢å¼•\n\n")
            f.write("> æœ¬è¯è¡¨ä»ã€Šå²è®°ã€‹å·²æ ‡æ³¨ç« èŠ‚ä¸­è‡ªåŠ¨æå–ï¼ŒåŒ…å«11ç±»å®ä½“è¯æ±‡ã€‚\n\n")

            # ç»Ÿè®¡ä¿¡æ¯
            total_entries = sum(len(vocab) for vocab in self.vocabularies.values())
            total_occurrences = sum(
                sum(len(contexts) for contexts in vocab.values())
                for vocab in self.vocabularies.values()
            )

            f.write("## ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- æ ‡æ³¨ç« èŠ‚æ•°ï¼š{len(list(self.chapter_dir.glob('*.tagged.md')))} ç¯‡\n")
            f.write(f"- è¯æ¡æ€»æ•°ï¼š{total_entries} ä¸ª\n")
            f.write(f"- æ ‡æ³¨æ€»æ•°ï¼š{total_occurrences} æ¬¡\n\n")

            f.write("## è¯è¡¨åˆ—è¡¨\n\n")
            f.write("| åºå· | ç±»åˆ« | æ ‡è®° | è¯æ¡æ•° | æ–‡ä»¶ |\n")
            f.write("|------|------|------|--------|------|\n")

            filename_map = {
                'äººå': '01_äººåè¯è¡¨.md',
                'åœ°å': '02_åœ°åè¯è¡¨.md',
                'å®˜èŒ': '03_å®˜èŒè¯è¡¨.md',
                'æ—¶é—´': '04_æ—¶é—´è¯è¡¨.md',
                'æœä»£': '05_æœä»£è¯è¡¨.md',
                'åˆ¶åº¦': '06_åˆ¶åº¦è¯è¡¨.md',
                'æ—ç¾¤': '07_æ—ç¾¤è¯è¡¨.md',
                'å™¨ç‰©': '08_å™¨ç‰©è¯è¡¨.md',
                'å¤©æ–‡': '09_å¤©æ–‡è¯è¡¨.md',
                'ç¥è¯': '10_ç¥è¯è¯è¡¨.md',
                'åŠ¨æ¤ç‰©': '11_åŠ¨æ¤ç‰©è¯è¡¨.md',
            }

            for i, (category, (_, marker)) in enumerate(ENTITY_PATTERNS.items(), 1):
                count = len(self.vocabularies[category])
                filename = filename_map[category]
                f.write(f"| {i} | {category} | `{marker}` | {count} | [{filename}](./{filename}) |\n")

            f.write("\n## ä½¿ç”¨è¯´æ˜\n\n")
            f.write("æ¯ä¸ªè¯è¡¨æ–‡ä»¶åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n")
            f.write("- **è¯æ¡åç§°**ï¼šæå–çš„å®ä½“è¯æ±‡\n")
            f.write("- **å‡ºç°æ¬¡æ•°**ï¼šè¯¥è¯æ¡åœ¨æ‰€æœ‰ç« èŠ‚ä¸­å‡ºç°çš„æ€»æ¬¡æ•°\n")
            f.write("- **å‡ºç°ç« èŠ‚**ï¼šåŒ…å«è¯¥è¯æ¡çš„ç« èŠ‚åˆ—è¡¨\n")
            f.write("- **å…¸å‹ç”¨ä¾‹**ï¼šè¯¥è¯æ¡çš„å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰\n\n")

            f.write("## æ ‡æ³¨è§„åˆ™\n\n")
            f.write("å®ä½“æ ‡æ³¨ä½¿ç”¨ç‰¹å®šç¬¦å·æ ‡è®°ï¼š\n\n")
            for category, (_, marker) in ENTITY_PATTERNS.items():
                f.write(f"- **{category}**ï¼š`{marker}è¯æ¡{marker}`\n")

        print(f"\n  ç´¢å¼•æ–‡ä»¶: README.md")

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("å²è®°åˆ†ç±»è¯è¡¨æ„å»ºå·¥å…·")
    print("="*60 + "\n")

    chapter_dir = "chapter_md"
    output_dir = Path("kg/vocabularies")

    # åˆ›å»ºæ„å»ºå™¨
    builder = VocabularyBuilder(chapter_dir)

    # æå–å®ä½“
    print("ç¬¬ä¸€æ­¥ï¼šä»æ ‡æ³¨æ–‡ä»¶ä¸­æå–å®ä½“\n")
    builder.process_all_files()

    # ç”Ÿæˆè¯è¡¨
    print("\nç¬¬äºŒæ­¥ï¼šç”Ÿæˆè¯è¡¨æ–‡ä»¶\n")
    builder.generate_all_vocabularies(output_dir)

    print("\n" + "="*60)
    print("è¯è¡¨æ„å»ºå®Œæˆï¼")
    print(f"è¾“å‡ºç›®å½•ï¼š{output_dir}")
    print("="*60 + "\n")

if __name__ == '__main__':
    main()
